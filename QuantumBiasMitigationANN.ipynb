{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OfhnPgIhV14G",
        "outputId": "355abed8-92f6-4f46-ff62-b820b26000bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class distribution:\n",
            " income\n",
            "0    0.75919\n",
            "1    0.24081\n",
            "Name: proportion, dtype: float64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
            "\n",
            "Model Comparison:\n",
            "Model                Accuracy   SPD       \n",
            "Baseline             0.8339     0.5173    \n",
            "Feature Selection    0.7916     0.5784    \n",
            "Regularization       0.7966     0.6036    \n",
            "Quantum Mitigation   0.8305     0.4993    \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import accuracy_score\n",
        "from imblearn.over_sampling import SMOTE  # For handling imbalance\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.regularizers import l2\n",
        "\n",
        "# Load and preprocess data\n",
        "df = pd.read_csv(\"income.csv\")\n",
        "X = df.drop(columns=[\"income\"])\n",
        "y = df[\"income\"]\n",
        "\n",
        "# Impute missing values\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "X = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)\n",
        "\n",
        "# Check class imbalance\n",
        "print(\"Class distribution:\\n\", y.value_counts(normalize=True))\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "y_train = y_train.to_numpy()\n",
        "y_test = y_test.to_numpy()\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Function to build ANN\n",
        "def build_ann(input_dim):\n",
        "    model = Sequential([\n",
        "        Dense(16, activation='relu', input_shape=(input_dim,)),\n",
        "        Dense(8, activation='relu'),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Function for fairness metric (SPD)\n",
        "def statistical_parity_difference(y_true, y_pred):\n",
        "    pos_rate_0 = np.mean(y_pred[y_true == 0])\n",
        "    pos_rate_1 = np.mean(y_pred[y_true == 1])\n",
        "    return abs(pos_rate_1 - pos_rate_0)\n",
        "\n",
        "# 1. Baseline ANN\n",
        "model_base = build_ann(X_train.shape[1])\n",
        "model_base.fit(X_train, y_train, epochs=50, batch_size=10, verbose=0, validation_data=(X_test, y_test))\n",
        "y_pred_base = (model_base.predict(X_test) > 0.5).astype(int)\n",
        "acc_base = accuracy_score(y_test, y_pred_base)\n",
        "spd_base = statistical_parity_difference(y_test, y_pred_base)\n",
        "\n",
        "# 2. Feature Selection with SMOTE\n",
        "selector = SelectKBest(score_func=mutual_info_classif, k=50)\n",
        "X_selected = selector.fit_transform(X, y)\n",
        "X_train_sel, X_test_sel, y_train_sel, y_test_sel = train_test_split(X_selected, y, test_size=0.2, random_state=42, stratify=y)\n",
        "smote = SMOTE(random_state=42)  # Oversample minority class\n",
        "X_train_sel_smote, y_train_sel_smote = smote.fit_resample(X_train_sel, y_train_sel)\n",
        "X_train_sel_smote = scaler.fit_transform(X_train_sel_smote)\n",
        "X_test_sel = scaler.transform(X_test_sel)\n",
        "model_sel = build_ann(X_train_sel_smote.shape[1])\n",
        "model_sel.fit(X_train_sel_smote, y_train_sel_smote, epochs=50, batch_size=10, verbose=0, validation_data=(X_test_sel, y_test_sel))\n",
        "y_pred_sel = (model_sel.predict(X_test_sel) > 0.5).astype(int)\n",
        "acc_sel = accuracy_score(y_test_sel, y_pred_sel)\n",
        "spd_sel = statistical_parity_difference(y_test_sel, y_pred_sel)\n",
        "\n",
        "# 3. Regularization with SMOTE\n",
        "X_train_reg_smote, y_train_reg_smote = smote.fit_resample(X_train, y_train)\n",
        "model_reg = Sequential([\n",
        "    Dense(16, activation='relu', input_shape=(X_train_reg_smote.shape[1],), kernel_regularizer=l2(0.01)),\n",
        "    Dense(8, activation='relu', kernel_regularizer=l2(0.01)),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "model_reg.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model_reg.fit(X_train_reg_smote, y_train_reg_smote, epochs=50, batch_size=10, verbose=0, validation_data=(X_test, y_test))\n",
        "y_pred_reg = (model_reg.predict(X_test) > 0.5).astype(int)\n",
        "acc_reg = accuracy_score(y_test, y_pred_reg)\n",
        "spd_reg = statistical_parity_difference(y_test, y_pred_reg)\n",
        "\n",
        "# 4. Enhanced Quantum Mitigation\n",
        "def hadamard_matrix(n):\n",
        "    H1 = np.array([[1, 1], [1, -1]]) / np.sqrt(2)\n",
        "    H = H1\n",
        "    for _ in range(n - 1):\n",
        "        H = np.kron(H, H1)\n",
        "    return H\n",
        "\n",
        "def oracle_matrix(n, target_indices):\n",
        "    size = 2**n\n",
        "    O = np.eye(size)\n",
        "    for idx in target_indices:\n",
        "        O[idx, idx] = -1\n",
        "    return O\n",
        "\n",
        "def diffuser_matrix(n):\n",
        "    size = 2**n\n",
        "    psi = np.ones(size) / np.sqrt(size)\n",
        "    D = 2 * np.outer(psi, psi) - np.eye(size)\n",
        "    return D\n",
        "\n",
        "# Larger subset and multiple iterations\n",
        "n_samples = 32  # 2^5 states\n",
        "n_qubits = 5\n",
        "subset_indices = np.random.choice(np.where(y_train == 1)[0], n_samples // 2)\n",
        "subset_indices = np.concatenate([subset_indices, np.random.choice(np.where(y_train == 0)[0], n_samples // 2)])\n",
        "X_subset = X_train[subset_indices]\n",
        "y_subset = y_train[subset_indices]\n",
        "target_indices = [i for i, label in enumerate(y_subset) if label == 1]\n",
        "\n",
        "# Grover's algorithm with 2 iterations\n",
        "state = np.ones(2**n_qubits) / np.sqrt(2**n_qubits)\n",
        "H = hadamard_matrix(n_qubits)\n",
        "O = oracle_matrix(n_qubits, target_indices)\n",
        "D = diffuser_matrix(n_qubits)\n",
        "for _ in range(2):  # 2 iterations for better amplification\n",
        "    state = H @ state\n",
        "    state = O @ state\n",
        "    state = H @ state\n",
        "    state = D @ state\n",
        "\n",
        "# Resample based on amplified probabilities\n",
        "probs = np.abs(state) ** 2\n",
        "amplified_indices = np.argsort(probs)[-len(target_indices):]\n",
        "X_train_quantum = np.vstack([X_train] + [X_subset[i] for i in amplified_indices for _ in range(5)])  # Duplicate 5x\n",
        "y_train_quantum = np.hstack([y_train] + [y_subset[i] for i in amplified_indices for _ in range(5)])  # Duplicate 5x\n",
        "\n",
        "model_quantum = build_ann(X_train_quantum.shape[1])\n",
        "model_quantum.fit(X_train_quantum, y_train_quantum, epochs=50, batch_size=10, verbose=0, validation_data=(X_test, y_test))\n",
        "y_pred_quantum = (model_quantum.predict(X_test) > 0.5).astype(int)\n",
        "acc_quantum = accuracy_score(y_test, y_pred_quantum)\n",
        "spd_quantum = statistical_parity_difference(y_test, y_pred_quantum)\n",
        "\n",
        "# Comparison\n",
        "print(\"\\nModel Comparison:\")\n",
        "print(f\"{'Model':<20} {'Accuracy':<10} {'SPD':<10}\")\n",
        "print(f\"{'Baseline':<20} {acc_base:<10.4f} {spd_base:<10.4f}\")\n",
        "print(f\"{'Feature Selection':<20} {acc_sel:<10.4f} {spd_sel:<10.4f}\")\n",
        "print(f\"{'Regularization':<20} {acc_reg:<10.4f} {spd_reg:<10.4f}\")\n",
        "print(f\"{'Quantum Mitigation':<20} {acc_quantum:<10.4f} {spd_quantum:<10.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hq6QRKO-V9CI"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6z5rxRXFWizP"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eZgtpXy4Wntr"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fDES6TiFXtIg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}